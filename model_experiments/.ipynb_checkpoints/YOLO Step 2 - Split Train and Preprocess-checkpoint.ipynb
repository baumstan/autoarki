{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import custom images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "#!pip3 install s3fs\n",
    "import s3fs\n",
    "\n",
    "my_bucket = 'autoarki-ground-truth-labeling'\n",
    "client = boto3.client('s3')\n",
    "response = client.list_objects_v2(Bucket=my_bucket)\n",
    "session = boto3.Session() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather list of annotated and non-annotated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all annotated files\n",
    "bucket = 'autoarki-ground-truth-labeling'\n",
    "dest = \"bounding_box/ground_truth_annots/yolo-bbox-train\"\n",
    "file_key = \"annot.csv\"\n",
    "annot_file = 's3://{}/{}/{}'.format(bucket, dest, file_key)\n",
    "df_ann = pd.read_csv(annot_file)\n",
    "\n",
    "annotated_img_list = list(df_ann.img_file.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all files that were reviewed in annotation job\n",
    "import s3fs\n",
    "\n",
    "reviewed_img_list = []\n",
    "\n",
    "mani_path = \"s3://autoarki-ground-truth-labeling/bounding_box/ground_truth_annots/yolo-bbox-train/manifests/output.manifest\"\n",
    "job_name = 'yolo-bbox-train'\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "with fs.open(mani_path, 'rb') as fin:    \n",
    "    annot_list = []\n",
    "\n",
    "    for line in fin.readlines():\n",
    "        record = json.loads(line)\n",
    "        if job_name in record.keys():  # is it necessary?\n",
    "            image_file_path = record[\"source-ref\"]\n",
    "            image_file_name = image_file_path.split(\"/\")[-1]\n",
    "            reviewed_img_list.append(image_file_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviewed files:  4206\n",
      "Number of annotated files:  140\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reviewed files: \", len(reviewed_img_list))\n",
    "print(\"Number of annotated files: \", len(annotated_img_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-annotated but reviewed files:  4066\n"
     ]
    }
   ],
   "source": [
    "# remove all annotated files from list\n",
    "non_annot_review_list = [x for x in reviewed_img_list if x not in annotated_img_list]\n",
    "print(\"Number of non-annotated but reviewed files: \", len(non_annot_review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-annotated but reviewed files to be used in train + test:  175\n"
     ]
    }
   ],
   "source": [
    "# select 125% of the annotated files \n",
    "\n",
    "import random\n",
    "seed = 123\n",
    "random.Random(seed).shuffle(non_annot_review_list)\n",
    "\n",
    "num_non_annotated = round(len(annotated_img_list)*1.25)\n",
    "\n",
    "non_ann_to_use_list = non_annot_review_list[:num_non_annotated]\n",
    "print(\"Number of non-annotated but reviewed files to be used in train + test: \", num_non_annotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split annotated and non-annotated data separately into train and test then append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(full_list, train_pct = 0.8):\n",
    "    '''Return list of files for train and list of files for test''' \n",
    "    split_1 = round(int(train_pct * len(full_list)),0)\n",
    "\n",
    "    train_filenames = full_list[ :split_1]\n",
    "    test_filenames = full_list[split_1: ]\n",
    "    \n",
    "    return train_filenames, test_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Files 252\n",
      "Number of Test Files 63\n"
     ]
    }
   ],
   "source": [
    "train_files, test_files = split_train_test(annotated_img_list) # annotated list\n",
    "non_ann_train, non_ann_test = split_train_test(non_ann_to_use_list)\n",
    "\n",
    "train_files.extend(non_ann_train)\n",
    "test_files.extend(non_ann_test)\n",
    "print(\"Number of Training Files\", len(train_files))\n",
    "print(\"Number of Test Files\", len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a subdirectory with copies of the files\n",
    "Note: Delete originals once proof of concept is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"training_data/\"\n",
    "test_dir = \"test_data/\"\n",
    "\n",
    "image_file_location = \"bounding_box/images/\"\n",
    "ann_text_file_location = 'bounding_box/ground_truth_annots/yolo-bbox-train/yolo_friendly_format/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "import botocore\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_files_for_YOLO(list_file_names, new_dir):\n",
    "    for i in list_file_names:\n",
    "        '''Copy over txt and jpg files. If the text file does not exist create a blank copy'''\n",
    "        \n",
    "        # get the txt file version of the name\n",
    "        add_txt = i.replace('.jpg', '.txt')\n",
    "\n",
    "        # identify old and new directories\n",
    "        image_old_key =  image_file_location + i\n",
    "        image_new_key = new_dir + i\n",
    "        txt_old_key = ann_text_file_location + add_txt\n",
    "        txt_new_key = new_dir + add_txt\n",
    "\n",
    "        # copy over the jpg\n",
    "        copy_source = {\n",
    "            'Bucket': my_bucket,\n",
    "            'Key': image_old_key\n",
    "        }\n",
    "        s3_resource.meta.client.copy(copy_source, my_bucket,image_new_key)\n",
    "        \n",
    "        # now repeat for txt. Create a txt file if it doesn't exist\n",
    "        copy_source_txt = {\n",
    "            'Bucket': my_bucket,\n",
    "            'Key': txt_old_key\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            s3_resource.Object(my_bucket, txt_old_key).load()\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                # The object does not exist.\n",
    "                # if the file doesn't already exist create an empty txt file\n",
    "                open(add_txt, 'w').close()\n",
    "                s3_resource.Bucket(my_bucket).upload_file(add_txt, txt_new_key)\n",
    "                os.remove(add_txt)\n",
    "\n",
    "            else:\n",
    "                # Something else has gone wrong.\n",
    "                raise\n",
    "        else:\n",
    "            # The object does exist. Then Copy it over\n",
    "            s3_resource.meta.client.copy(copy_source_txt, my_bucket,txt_new_key)\n",
    "            \n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_files_for_YOLO(train_files, train_dir)\n",
    "copy_files_for_YOLO(test_files, test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use smallest YOLO model to start yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchvision \n",
    "#!conda install pytorch torchvision -c pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 26, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "# change directory\n",
    "import os\n",
    "os.chdir(r\"/root/AutoArki/yolov5/yolov5\")\n",
    "! python train.py\\\n",
    "    --data AutoArki/kineret - /root/AutoArki/kineret - AWS experiments/custom_dataset.yaml \\\n",
    "    --epochs 30 \\\n",
    "    --project custom_yolov5 \\\n",
    "    --bbox_interval 1 \\\n",
    "    --save_period 1 \\\n",
    "    --weights yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_dataset = KangarooDataset()\n",
    "train_dataset.load_dataset(dataset_dir='./kangaroo-transfer-learning/kangaroo', is_train=True)\n",
    "train_dataset.prepare()\n",
    "\n",
    "# Validation\n",
    "validation_dataset = KangarooDataset()\n",
    "validation_dataset.load_dataset(dataset_dir='./kangaroo-transfer-learning/kangaroo', is_train=False)\n",
    "validation_dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KangarooConfig(mrcnn.config.Config):\n",
    "    NAME = \"kangaroo_cfg\"\n",
    "\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    \n",
    "    NUM_CLASSES = 2\n",
    "    \n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    STEPS_PER_EPOCH = 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
